**********************************************************************************************************************
                                  Dimensionality Reduction Technique
**********************************************************************************************************************
Visualizing and drawing inferences from high dimensions data is more challenging.

Advantages:
    Handle high dimensions (features) data    
    We can quickly extract patterns and insights from it
    Reduce the number of features in dataset without having to lose much information and keep (or improve) the model’s performance.
    Reduce computation time and training time
    Remove redundant feature 


Methods for Dimension Reduction:
	1. Feature Selection : By only keeping the most relevant variables from the original dataset.
    2. Feature Reduction : By finding a smaller set of new variables, each being a combination of the input variables, 
                           containing basically the same information as the input variables.


1. Feature Selection Technique :
	Missing Value Ratio
	Low Variance Filter
	High Correlation Filter
	Random Forest
	Backward Feature Extraction
	Forward Feature Selection

2. Feature Reduction Technique :
	Factor Analysis
	Principle Component Analysis (PCA)
	Independent Component Analysis
	Singular Value Decomposition (SVD)



Missing Value Ratio:
	Find the features having missing value.
	Analyse the possible reasons of missing value.
	Impute the missing value or drop the feature if missing value ratio is too high.
	Before droping a feature consider its significance in problem domain. 

	# Import libraries
	import pandas as pd
	import numpy as np

	# Load the data
	data = pd.read_csv(‘file_location’)

	# Find the ratio of missing value of each feature
	missing_ratio = data.isnull().sum()/len(train)*100

	# Iterate through all the columns/features and drop feature having high missing ratio
	variables = data.columns
	variable = variables[missing_ratio < threshold_Value]

	# Impute techniques


Low Variance Filter:
	Impute all missing value.
	Calculate the variance of numerical Variables
	Drop low variance variable as it will not affect the target variable.
	Like if a variable has only value 1, so it will not affect the target variable.

	# Import Library
	import numpy as np
	import pandas as pd

	# Load data
	data = pd.read_csv(‘file_location’)

	# Impute variables having missing value
	# Check whether missing value is imputed or not
	data.isnull().sum()/len(train)*100 should be 0.

	# Find variance of each variables
	vars = data.var()

	# Iterate through all the columns/features and drop feature having low variance
	variables = data.coloumns
	variable = variables[vars < threshold_value]


High Correlation Filter:
	High correlation between two variables means they have similar trends and are likely to carry similar information.
    Calculate the correlation between independent numerical variables
    Keep those variables which shows a decent or high correlation with the target variable.

    # Import Library
	import numpy as np
	import pandas as pd

	# Load data
	data = pd.read_csv(‘file_location’)

	# Find correlation
	cor = data.corr()

	# drop variable having low correlation with target variable



Random Forest:
	Select Numerical Variables
	Train a Random Forest Model
	Visualize the importance of each variable with target variable 

	# Import libraries
	import numpy as np
	import pandas as pd
	from sklearn.ensemble import RandomForestRegressor
	import matplotlib.pyplot as plt

	# Load the data
	data = pd.read_csv("/home/maneeshp626/Desktop/Ecommerce.csv")

	# Drop non-numerical variables and target variable
	df=data.drop(['Yearly_Amount_Spent','Address','Avatar', 'Email'], axis=1)

	# Create Model and Train the model with dummy data
	model = RandomForestRegressor(random_state=1, max_depth=5)
	df=pd.get_dummies(df)
	model.fit(df, data.Yearly_Amount_Spent)
	features = df.columns
	importances = model.feature_importances_
	indices = np.argsort(importances)[-3:]  # Top 4 features

	# Draw Plot of Feature Importance
	plt.title('Feature Importances')
	plt.barh(range(len(indices)), importances[indices], color='b', align='center')
	plt.yticks(range(len(indices)), [features[i] for i in indices])
	plt.xlabel('Relative Importance')
	plt.show()



Backward Feature Elimination :
	Steps:
    	We first take all the n variables present in our dataset and train the model using them We then calculate the performance of the model.
    	Now, we compute the performance of the model after eliminating each variable (n times), i.e., we drop one variable every time and train the model on the remaining n-1 variables.
    	We identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable.
    	Repeat this process until no variable can be dropped.


Forward Feature Selection :
	Steps:
    	We start with a single feature. Essentially, we train the model n number of times using each feature separately.
    	The variable giving the best performance is selected as the starting variable.
    	Then we repeat this process and add one variable at a time. The variable that produces the highest increase in performance is retained.
    	We repeat this process until no significant improvement is seen in the model’s performance.









Use Case:

	Missing Value Ratio           : If the dataset has too many missing values, We can drop the variables having a large 
	                                number of missing values in them.
    Low Variance filter           : To identify and drop constant variables from the dataset. The target variable is not 
                                    unduly affected by variables with low variance, and hence these variables can be safely dropped.
    High Correlation filter       : A pair of variables having high correlation increases multicollinearity in the 
                                    dataset. So, we can use this technique to find highly correlated features and drop them accordingly.
    Random Forest                 : We can find the importance of each feature present in the dataset and keep the top 
                                    most features
    Backward Feature Elimination  : 
    Forward Feature Selection     : Both take a lot of computational time and are thus generally used on smaller 
                                    datasets.
    Factor Analysis               : This technique is best suited for situations where we have highly correlated set of 
                                    variables. It divides the variables based on their correlation into different groups, and represents each group with a factor
    Principal Component Analysis  : Widely used techniques for dealing with linear data. It divides the data into a set 
                                    of components which try to explain as much variance as possible.
    Independent Component Analysis: We can use ICA to transform the data into independent components which describe the 
                                    data using less number of components


Reference:
https://www.analyticsvidhya.com