**********************************************************************************************************************
                                  Dimensionality Reduction Technique
**********************************************************************************************************************
Visualizing and drawing inferences from high dimensions data is more challenging.

Advantages:
    Handle high dimensions (features) data    
    We can quickly extract patterns and insights from it
    Reduce the number of features in dataset without having to lose much information and keep (or improve) the model’s performance.
    Reduce computation time and training time
    Remove redundant feature 


Methods for Dimension Reduction:
	1. Feature Selection : By only keeping the most relevant variables from the original dataset.
    2. Feature Reduction : By finding a smaller set of new variables, each being a combination of the input variables, 
                           containing basically the same information as the input variables.


1. Feature Selection Technique :
	Missing Value Ratio
	Low Variance Filter
	High Correlation Filter
	Random Forest
	Backward Feature Extraction
	Forward Feature Selection

2. Feature Reduction Technique :
	Factor Analysis
	Principle Component Analysis (PCA)
	Independent Component Analysis
	Singular Value Decomposition (SVD)



Missing Value Ratio:
	Find the features having missing value.
	Analyse the possible reasons of missing value.
	Impute the missing value or drop the feature if missing value ratio is too high.
	Before droping a feature consider its significance in problem domain. 

	# Import libraries
	import pandas as pd
	import numpy as np

	# Load the data
	data = pd.read_csv(‘file_location’)

	# Find the ratio of missing value of each feature
	missing_ratio = data.isnull().sum()/len(train)*100

	# Iterate through all the columns/features and drop feature having high missing ratio
	variables = data.columns
	variable = variables[missing_ratio < threshold_Value]




Low Variance Filter:
	Impute all missing value.
	Calculate the variance of numerical Variables
	Drop low variance variable as it will not affect the target variable.
	Like if a variable has only value 1, so it will not affect the target variable.

	# Import Library
	import numpy as np
	import pandas as pd

	# Load data
	data = pd.read_csv(‘file_location’)

	# Impute variables having missing value
	# Check whether missing value is imputed or not
	data.isnull().sum()/len(train)*100 should be 0.

	# Find variance of each variables
	vars = data.var()

	# Iterate through all the columns/features and drop feature having low variance
	variables = data.coloumns
	variable = variables[vars < threshold_value]


High Correlation Filter:
	High correlation between two variables means they have similar trends and are likely to carry similar information.
    Calculate the correlation between independent numerical variables
    Keep those variables which shows a decent or high correlation with the target variable.

    # Import Library
	import numpy as np
	import pandas as pd

	# Load data
	data = pd.read_csv(‘file_location’)

	# Find correlation
	cor = data.corr()

	# drop variable having low correlation with target variable



Random Forest:
	Select Numerical Variables
	Train a Random Forest Model
	Visualize the importance of each variable with target variable 

	# Import libraries
	import numpy as np
	import pandas as pd
	from sklearn.ensemble import RandomForestRegressor
	import matplotlib.pyplot as plt

	# Load the data
	data = pd.read_csv("/home/maneeshp626/Desktop/Ecommerce.csv")

	# Drop non-numerical variables and target variable
	df=data.drop(['Yearly_Amount_Spent','Address','Avatar', 'Email'], axis=1)

	# Create Model and Train the model with dummy data
	model = RandomForestRegressor(random_state=1, max_depth=5)
	df=pd.get_dummies(df)
	model.fit(df, data.Yearly_Amount_Spent)
	features = df.columns
	importances = model.feature_importances_
	indices = np.argsort(importances)[-3:]  # Top 4 features

	# Draw Plot of Feature Importance
	plt.title('Feature Importances')
	plt.barh(range(len(indices)), importances[indices], color='b', align='center')
	plt.yticks(range(len(indices)), [features[i] for i in indices])
	plt.xlabel('Relative Importance')
	plt.show()



Backward Feature Elimination :
	Steps:
    	We first take all the n variables present in our dataset and train the model using them We then calculate the performance of the model.
    	Now, we compute the performance of the model after eliminating each variable (n times), i.e., we drop one variable every time and train the model on the remaining n-1 variables.
    	We identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable.
    	Repeat this process until no variable can be dropped.


Forward Feature Selection :
	Steps:
    	We start with a single feature. Essentially, we train the model n number of times using each feature separately.
    	The variable giving the best performance is selected as the starting variable.
    	Then we repeat this process and add one variable at a time. The variable that produces the highest increase in performance is retained.
    	We repeat this process until no significant improvement is seen in the model’s performance.


Principle Component Analysis:
	PCA is a technique which helps us in extracting a new set of variables from an existing large set of variables. These newly extracted variables are called Principal Components. A principal component is a normalized linear combination of the original predictors in a data set. Normalized predictors have mean equals to zero and standard deviation equals to one.

	Steps to compute PCA:
		1. Standardization :
			Standardize the range of the continuous variables so that each one of them contributes equally to the analysis. PCA is quite sensitive regarding the variance of the initial variables. Variables with larger ranges will dominate over those with small ranges. 

				Standardization(Z) = (value-mean)/std

			When difference in the range of the variables it high we can use standardization. If model is sensitive to the magnitude of variance, standardization may be needed.

		2. Covariance Matrix computation :
			The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. 

			The entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. Since
												Cov(a,a) = Var(a)
												Cov(a,b) = Cov(b,a)

			Positive Corr : The two variables increase or decrease together (correlated)
      		Negative Corr : One increases when the other decreases (Inversely correlated)
      		Zero Corr : No correlation

      	3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components :
      		N-dimension data gives n PCAs. New variables are uncorrelated. 

      		An important thing is that principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables.

			Geometrically, principal components represent the directions of the data that explain a maximal amount of variance, means the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has.

			'How PCA constructs the Principal Components?'

			Let’s go back to eigenvectors and eigenvalues. Their number is equal to the number of dimensions of the data. For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues.

			Because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance (most information) and that we call Principal Components. And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. 

			By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance.


		4. Feature vector:
			Select the features according to the significance of eigen value.	


		5. Recast the data along the principal components axes :
			In this step, use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis).

			This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.

				Final_dataset = T(Feature_Vector) * T(Standardized_Original_dataset) 


Standardization Use Case:
	When difference in the range of the dataset it high we can use standardization. If model is sensitive to the magnitude of variance, standardization may be needed.

	Before PCA:
		In Principal Component Analysis, features with high variances/wide ranges, get more weight than those with low variance, and consequently, they end up illegitimately dominating the First Principal Components.
		Standardization can prevent this, by giving same wheightage to all features.

	Before Clustering:
		Clustering models are distance based algorithms, in order to measure similarities between observations and form clusters they use a distance metric. So, features with high ranges will have a bigger influence on the clustering. Therefore, standardization is required before building a clustering model.

	Before KNN:
		k-nearest neighbors is a distance based classifier that classifies new observations based on similarity measures with labeled observations of the training set. Standardization makes all variables to contribute equally to the similarity measures.

	Before SVM :
		Support Vector Machine tries to maximize the distance between the separating plane and the support vectors. If one feature has very large values, it will dominate over other features when calculating the distance. So Standardization gives all features the same influence on the distance metric.

	Before measuring variable importance in regression models :
		You can measure variable importance in regression analysis, by fitting a regression model using the standardized independent variables and comparing the absolute value of their standardized coefficients. But, if the independent variables are not standardized, comparing their coefficients becomes meaningless.



Dimension Reduction Use Case:

	Missing Value Ratio           : If the dataset has too many missing values, We can drop the variables having a large 
	                                number of missing values in them.
    Low Variance filter           : To identify and drop constant variables from the dataset. The target variable is not 
                                    unduly affected by variables with low variance, and hence these variables can be safely dropped.
    High Correlation filter       : A pair of variables having high correlation increases multicollinearity in the 
                                    dataset. So, we can use this technique to find highly correlated features and drop them accordingly.
    Random Forest                 : We can find the importance of each feature present in the dataset and keep the top 
                                    most features
    Backward Feature Elimination  : 
    Forward Feature Selection     : Both take a lot of computational time and are thus generally used on smaller 
                                    datasets.
    Factor Analysis               : This technique is best suited for situations where we have highly correlated set of 
                                    variables. It divides the variables based on their correlation into different groups, and represents each group with a factor
    Principal Component Analysis  : Widely used techniques for dealing with linear data. It divides the data into a set 
                                    of components which try to explain as much variance as possible.
    Independent Component Analysis: We can use ICA to transform the data into independent components which describe the 
                                    data using less number of components







Reference:
https://www.analyticsvidhya.com
https://towardsdatascience.com